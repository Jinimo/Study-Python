## 1. Data Definition

1. Train / Test Dataset
2. 가설(Hypothesis) 수립 
3. 비용 함수 (Cost Function)
4. Optimizer - 경사 하강법 (Gradient Descent)



### 02. 가설(Hypothesis) 수립 

> 머신러닝에서 식 세우는 경우 의미

- 임의로 추측하여 세운 식
- 경험적으로 알고있는 식
- 가설 기각시 수정해 나아가는 식



➡️ **선형 회귀의 가설** :학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 과정 (직선 방정식)

H: Hypothesis

W: weight (가중치) -> 기울기

b: bias (편향) -> y 절편

`y= Wx+b` or `H(x)= Wx+b` 



### 03. 비용 함수 (Cost Function)

> 정답에 대한 오류 나타냄.
>
> 정답에 가까울 수록 작은값, 오답에 가까울 수록 큰값

**비용 함수(cost function)** = **손실 함수(loss function)** = **오차 함수(error function)** = **목적 함수(objective function)**



⭐️ **비용함수(Cost Function) 최소화**

* 회귀분석의 목표: 비용함수의 최소화

-> `Cost(W,b)`의 최소값으로 만드는 `W`, `b`를 구하면 훈련 데이터를가장 잘 나타내는 직선 구할 수 있음.

<img src="https://t1.daumcdn.net/cfile/tistory/99C4EA3E5C79EBD81A" alt="img" style="zoom:40%;" />



➡️ **오차(error)**: 각 예측값과 실제값의 동일한 x값에서의 y값 차이 = 실제값에서의 오차

➡️ **평균제곱오차 (MSE, Mean Squared Error)**

yi: 신경망의 출력값 (추정값) 

ti: 정답 레이블

i: 데이터의 차원 수 

<img src="https://user-images.githubusercontent.com/10937193/58108494-e7bc7580-7c26-11e9-90a1-b988522a0b64.png" alt="스크린샷 2019-05-22 오전 12 11 48" style="zoom:20%;" />

➡️ **교차 엔트로피 오차 (CEE, Cross Entropy Error)**

<img src="https://user-images.githubusercontent.com/10937193/58108191-577e3080-7c26-11e9-8b54-097fec3e5f0e.png" alt="img" style="zoom:20%;" />



예시)

<img src="https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC1.PNG" alt="img" style="zoom:33%;" /><img src="https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC2.PNG" alt="img" style="zoom:33%;" /><img src="https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC3.PNG" alt="img" style="zoom:33%;" />

임의의 직선 y = 13x + 1(예측값), 점 4개(실제값)

| hours(x) | 2    | 3    | 4    | 5    |
| :------- | :--- | :--- | :--- | :--- |
| 실제값   | 25   | 50   | 42   | 61   |
| 예측값   | 27   | 40   | 53   | 66   |
| 오차     | -2   | 10   | -9   | -5   |

MSE = ((-2)^2 + (10)^2 + (-9)^2 + (-5)^2 )/4 = 52.5

-> 예측값과 실제값의 평균 제곱 오차의 값 = 52.5



### 04. Optimizer - 경사 하강법 (Gradient Descent)

**Optimizer Algorithm**: 최적화 알고리즘

**Gradient Descent** : 경사를 따라 내려가 최적의 값을 찾는 최적화 알고리즘 중 하나

	- 장점: 어떠한 점에서 출발 하더라도 최종적으로 기울기가 0인 값을 찾음
	- 실제 학습 진행시 W 값을 아무 점에서 시작하여 cost 값을 줄어들게끔 반복하여 W 값 찾음
	- `W`와` b`에 대해서 동시에 경사 하강법을 수행하면서 최적의 `W`와 `b`의 값을 찾아감

<img src="https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG" alt="img" style="zoom:60%;" />

- 경사도 (기울기, weight): `W` -><img src="https://t1.daumcdn.net/cfile/tistory/99E9EC405C79EBD81C" alt="img" style="zoom:50%;" />

<img src="https://t1.daumcdn.net/cfile/tistory/99FB6A4C5C79EBD814" alt="img" style="zoom:50%;" />

미분하여 결과값을 쉽게 보기위해 1/m -> 1/2m, 결과값에 큰 영향 X

- 경사도 미분 

알파(`α`): Learning rate. ` W`값의 수정 반복이 진행될 때 다음 지점을 어느정도 옮겨야 하는지 결정하는 상수

- 기울기 음수인 경우: `W`값 증가

`W := W - α × (음수 기울기) = W + α × (양수 기울기)`

- 기울기 양수인 경우: W값 감소

`W := W - α × (양수 기울기)`



<img src="https://t1.daumcdn.net/cfile/tistory/99CA223D5C79EBD814" alt="img" style="zoom:50%;" />

지나치게 `α` (Learning rate) 값을 증가시키는 경우 

-> 선의 기울기가 0이 되는 `W`를 찾아가는 것이 아니라 `W`의 값이 발산하는 상황을 보여줌

반대로 학습률 α가 지나치게 낮을 경우

-> 학습 속도가 느려짐. 적당한 `α`의 값을 찾아내는 것도 중요함



---

[참고]

- https://myjamong.tistory.com/83
- https://wikidocs.net/53560
