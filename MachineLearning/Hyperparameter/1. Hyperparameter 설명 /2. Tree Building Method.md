### Hyperparameter 설명 



#### 2) Tree Building Method

> - Tree 생성하는 방법 설정 parameter
> - Xgboost에만 존재. 모델 성능과 학습시간에 영향 줌 
> - 정확한 학습을 위해 Exact method (`exact`), 학습 속도 높이기 위해 Approximate method( `approx`, `hist`, `gpu_hist`) 이용하는 것이 좋음 



▶️ 사용법

- 데이터 행의 수가 10만 단위가 넘어간다면, xgboost에서는 `gpu_hist`를 쓰는 것이 좋음.
- 데이터 행의 수가 적다면, 정확도를 위해 `exact` 사용, 
- 데이터 행수가 많은데,
  - `gpu`를 사용 불가 →  `hist`
  - `gpu`를 사용 가능 →   `gpu_hist`



✅ 비교 및 설명 

**Tree Building Method**

- xgboost에만 존재하는 parameter → `tree_method`

- `exact`, `approx`, `hist`, `gpu_hist`(gpu를 사용할 수 있는 경우에만)를 선택 가능

  - `exact`

    - tree split을 위해 모든 데이터 point를 고려하는 방법
    - 장점) 정확한 split 가능.
    -  단점) 분산처리 불가능하여 학습 속도 느림

  - `approx`, `hist`, `gpu_hist`

    - tree split을 위해 모든 데이터 point를 고려 X
    - 분수위나 histogram 기준 데이터 분할(=버킷을 만든다)하여 split 수행 한 후, information gain이 가장 높은 버킷의 split을 최종 선택
    - 차이점
      - `approx` : loss function 업데이트에 loss function의 hessian(loss function의 second-order partial derivatives)을 사용
      -  `hist`,  `gpu_hist` : 사전에 지정한 값을 사용
      - lghtgbm의 학습 방법이 `hist`인데, 구현 방법에는 차이점 존재. ([[ref] xgboost 공식문서](https://xgboost.readthedocs.io/en/latest/treemethod.html#approximated-solutions))

    

---

[참조]

[XGBoost와 LightGBM 하이퍼파라미터 튜닝 가이드 by psystat](https://psystat.tistory.com/131#head2)