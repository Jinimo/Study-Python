### Hyperparameter 설명



#### 3) Learning rate

> - boosting 각 iteration 마다 곱해지는 가중치(loss function의 step size), 모형 성능과 학습시간에 영향 줌
> - 보통 작을수록 모형 성능 향상에 도움. 하지만, 학습 시간 길어지는 trade-off 
> - Hyperparameter 튜닝 시, 0.1~0.3 정도의 값 사용. 
> - 최종 모형 학습 시 0.05이하의 값을 사용하는 것이 좋음.



▶️ 사용법

- `Learning rate`를 optimizer에 넣어 튜닝할 필요는 X 
- 주어진 컴퓨팅 환경을 고려하여 적당한 값을 정한 후, 그 값에서 다른 parameter들을 튜닝하는 것이 좋음 
- `optimizer`에 `Learning rate`를 포함시켜 0.0202048 같은 값에 over fitting 시키는 것은 불필요한 시도

- 최종 학습 시
  -  0.05 이하의 값 사용하여 모형 성능 향상에 초점 맞추기
- Hyperparameter tunning
  - 0.1 이상의 값 사용하여 학습 속도 높이기
- 단, `Learning rate`를 크게 잡았을 떄 모형 적합이 잘 되지 X면 → 이 값을 기준으로 튜닝한 다른 Hyperparameter의 조합이 `learning rate`를 낮추었을 때는 최적 조합이 아니게 될 가능성 높음 



✅ 비교 및 설명

- XGBoost 

|                 | XGBoost         | Lightgbm        |
| --------------- | --------------- | --------------- |
|                 | `eta`           | `learning_rate` |
| sckit-learn API | `learning_rate` | `learning_rate` |
| default value   | 0.3             | 0.1             |

- 학습 속도 차이 반영에 의해 default 값 다른 것으로 보임 



---

[참조]

[XGBoost와 LightGBM 하이퍼파라미터 튜닝 가이드 by psystat](https://psystat.tistory.com/131#head2) 