### Hyperparameter 설명



#### 5) Maximum Depth

> - 가장 먼저 tunning해야 할 hyperparameter
> - 각 tree의 최대 깊이
> - 최대 리프 수(*Maximum Leaves*), 모형 성능, 학습 시간에 영향 줌
> - 클수록 
>   - fit well to the training data 
>   - 하지만, over fitting 가능성도 높아짐 
> - 보통 3 ~ 12의 값 사용 



▶️ 사용법

- gradient boosting에 가장 민감한 hyperparameter → `max_depth`를 가장 먼저 tunning하는 것이 좋음 

- 데이터의 복잡도에 따라 필요한 tree depth가 다름 

  - 너무 **크게** 잡으면, data over fitting
  - 너무 **작게** 잡으면, 학습이 제대로 안되는 under fitting

  

- lightgbm 같이 leaf-wise로 학습하는 경우 

  - `max_depth = -1`로 설정하는 것이 효과적

- XGBoost의 경우 `max_depth`에 따른 성능 차이 큰 경우 많음 



- *Maximum Leaves = (2^depth)-1* 
  - 예를 들어, Maximum Depth=10인 경우 Maximum Leaves=1023



✅ 비교 및 설명

|                 |   XGBoost   |  Lightgbm   |
| :-------------: | :---------: | :---------: |
|                 | `max_depth` | `max_depth` |
| sckit-learn API |   `동일`    |   `동일`    |
|  Default value  |      6      |     -1      |
| Infinite depth  |      0      |     -1      |

- Infinite depth (가능한 최대 tree split)
- XGBoost에서 GPU 사용 시 (`tree_method='gpu_hist'`)
  - 최대 16까지 설정 가능 



---

[참조]

[XGBoost와 LightGBM 하이퍼파라미터 튜닝 가이드 by psystat](https://psystat.tistory.com/131#head2) 